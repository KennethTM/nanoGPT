{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPT, GPTConfig\n",
    "import torch\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(model_name)\n",
    "\n",
    "def encode(string):\n",
    "    return tokenizer.encode(string, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "def decode(string):\n",
    "    return tokenizer.decode(string)\n",
    "\n",
    "def encode_tensor(string, device):\n",
    "    return torch.tensor(encode(string), dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nanogpt model (master branch)\n",
    "#model = GPT.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#run generation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(prompt_tensor, \u001b[38;5;241m10\u001b[39m, greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m completion_tokens \u001b[38;5;241m=\u001b[39m completion[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch310/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "# baseline model (master branch)\n",
    "model = GPT(GPTConfig(vocab_size=50257))\n",
    "model.load_state_dict(torch.load('models/gpt2.pth'))\n",
    "model.to(device, dtype=torch.bfloat16)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Hello, my name is Martin\"\n",
    "prompt_tensor = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "#run generation\n",
    "start_time = time.time()\n",
    "completion = model.generate(prompt_tensor, 10, greedy=True)\n",
    "end_time = time.time()\n",
    "\n",
    "completion_tokens = completion[0].cpu().tolist()\n",
    "num_tokens = len(completion_tokens)\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "tokens_per_second = (num_tokens-len(prompt_tensor.squeeze(0))) / time_taken\n",
    "\n",
    "print(decode(completion_tokens))\n",
    "print(f\"Number of tokens: {num_tokens}\")\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "print(f\"Tokens per second: {int(tokens_per_second)}\")\n",
    "print(completion)\n",
    "\n",
    "#float32\n",
    "#Hello, my name is Martin. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times.\n",
    "#bfloat16\n",
    "#Hello, my name is Martin. I am a member of the United States Army, and I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am\n",
    "\n",
    "#baseline is 200 tokens per second (210 int8), 180 with sampling (160 int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "from generate import load_model\n",
    "\n",
    "precision = torch.bfloat16\n",
    "device=\"cuda\"\n",
    "model_path = \"models/gpt2.pth\"\n",
    "model = load_model(model_path, device, precision, strict=True)\n",
    "\n",
    "#model_path_8int = \"models/gpt2-int8.pth\"\n",
    "#model = load_model(model_path_8int, device, precision, strict=False)\n",
    "\n",
    "prompt = \"Wikis are enabled by wiki software, otherwise known as wiki engines. A wiki engine, being a form of a content management system, differs from other web-based systems such as blog software or static site generators, in that the content is created without any defined owner or leader, and wikis have little inherent structure, allowing structure to emerge according to the needs of the users.\"\n",
    "prompt_tensor = encode_tensor(prompt, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 250\n",
      "Time taken: 864.56 seconds\n",
      "Tokens per second: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([33010,   271,   389,  9343,   416, 22719,  3788,    11,  4306,  1900,\n",
       "           355, 22719, 11874,    13,   317, 22719,  3113,    11,   852,   257,\n",
       "          1296,   286,   257,  2695,  4542,  1080,    11, 24242,   422,   584,\n",
       "          3992,    12,  3106,  3341,   884,   355,  4130,  3788,   393,  9037,\n",
       "          2524, 27298,    11,   287,   326,   262,  2695,   318,  2727,  1231,\n",
       "           597,  5447,  4870,   393,  3554,    11,   290, 47145,   271,   423,\n",
       "          1310, 11519,  4645,    11,  5086,  4645,   284, 14740,  1864,   284,\n",
       "           262,  2476,   286,   262,  2985,    13,   198,   198,  6425,  1165,\n",
       "           326,   612,   389,   867,  1180,  5107,   286, 47145,   271,   290,\n",
       "           484,   477, 13238,  4622,   287,  2846,   286,  2846,   286,   703,\n",
       "           511,  1321,   318,  5257,    11,   355,   880,   355,   287,   262,\n",
       "          2695,   345,   460,  4370,    13,  1881,  4465,  7989,   318,   284,\n",
       "         32781,   530,   355,   257, 22719,  5464,   357,  4758,   318,   635,\n",
       "          1900,   355,   257, 22719,  5464,   828,   290,  1194,   355,   257,\n",
       "         47145,   271, 18382,   357,  4758,   318,   635,  1900,   355,   257,\n",
       "         47145,   271, 18382,   737,   198,   198, 33010,   271,   389,  6986,\n",
       "           281,  1280,  2723, 22719,   290,  4145,   389,  6768,   973,   416,\n",
       "          3925,    11,  9611,    11,  3707,  6712,   290,   584, 22447,    13,\n",
       "         13078, 15719,   389,  3221,  5140,   287,   262,   749,  9208,  6116,\n",
       "           286,  4588,   286,   262,  4318,  4934,    11,   290,   389,  4143,\n",
       "          2950,   287, 11149,   262,  2695,   286,   262, 22719,    11,   477,\n",
       "           262,   981, 13359,   326,   262, 22719,   318,  9456,   355,   340,\n",
       "           815,   307,    13,   198,   198, 33010,   271,  2291,   257,  4996,\n",
       "           286, 15719,   290, 10233,    11,  1690,   416, 19771,  2972,  4899,\n",
       "           290,  4899,  1978,   284,  2251,    11,  4370,    11,   290,  5529,\n",
       "           257, 22719,    13,   198,   198, 32603, 15719,   423,   257,  3741,\n",
       "           286,   262,  2695,    11,   290,   867,  3033,    11,   287,   511,\n",
       "          1630,    13,  4042, 15719,   423,   281, 13684,  5798,   284,  4155,\n",
       "           477,   262,  2695,   286,   262, 22719,   318, 17232,    11,   290,\n",
       "           635,   284,  4155,   326,   484,   389, 16689,   284,   511,  2985,\n",
       "           357,   292,  6886,   284,   262, 15719,  2405,   737,   383, 15719,\n",
       "           423,  1844,  4934,   625,   262, 22719,    11,   351,   262,  5798,\n",
       "           329,   262,  2695,    11,   262, 15719], device='cuda:0'),\n",
       " {'accept_counts': [249, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate import run_generation\n",
    "#https://github.com/huggingface/transformers/issues/25420\n",
    "\n",
    "run_generation(prompt_tensor,\n",
    "               model, \n",
    "               250, \n",
    "               prompt_lookup=True,\n",
    "               compile=False, \n",
    "               compile_prefill=False,\n",
    "               temperature=0.7,\n",
    "               top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 250\n",
      "Time taken: 0.66 seconds\n",
      "Tokens per second: 378\n",
      "\n",
      "\n",
      "All content is shared, but not necessarily indexed, and Wikis are expected to provide information as to the content's status and status as it is published, but not necessarily when it is published.\n",
      "\n",
      "Wikis are self-contained and can be accessed, edited, or viewed from any computer.\n",
      "\n",
      "Wikis are the most popular in the world, but are also fairly unknown in the United States, Europe, and Asia.\n",
      "\n",
      "Wikis are based on the Wikia language, which is the official document of all Wikis, and are used to generate, organize, and maintain the Wikia wiki and wiki files, and to support wiki maintenance and the maintenance of the wiki.\n",
      "\n",
      "The Wikipedia wiki is hosted on the wiki server site. The wiki is not hosted on the internet, and the server is not accessed from anywhere.\n",
      "\n",
      "Wikis are not limited to websites, but include the following:\n",
      "\n",
      "The wiki server\n",
      "\n",
      "the Wikipedia web site\n",
      "\n",
      "the Wikipedia Wikian\n",
      "\n",
      "The wiki is hosted on the wiki server site hosted on the server of the wiki server, known as the server of the wiki.\n",
      "\n",
      "For information on the Wiki, see: Wiki.\n",
      "\n",
      "The Wikipedia server\n",
      "\n",
      "{'accept_counts': [249, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "output, metrics = run_generation(prompt_tensor, \n",
    "                        model, \n",
    "                        250, \n",
    "                        prompt_lookup=True,\n",
    "                        compile=True, \n",
    "                        compile_prefill=True,\n",
    "                        temperature=0.7,\n",
    "                        top_k=50)\n",
    "\n",
    "prompt_len = len(prompt_tensor)\n",
    "\n",
    "print(decode(output.cpu().tolist()[prompt_len:]))\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_candidate_pred_tokens(input_ids: torch.Tensor, max_ngram_size: int = 3, num_pred_tokens: int = 10) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Finds candidate prediction tokens based on the input_ids.\n",
    "\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): The input tensor of shape (batch_size, seq_len) containing token IDs.\n",
    "        max_ngram_size (int, optional): The maximum size of the n-gram to search for. Defaults to 3.\n",
    "        num_pred_tokens (int, optional): The number of prediction tokens to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The tensor containing the candidate prediction tokens.\n",
    "    \"\"\"\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:end_idx]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='mps:0', dtype=torch.int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_candidate_pred_tokens(prompt_tensor, max_ngram_size=3, num_pred_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_find_candidate_pred_tokens():\n",
    "\n",
    "    # Test Case 1: Matching ngram is found\n",
    "    with_match = torch.tensor([1, 2, 3, 1, 2, 3, 1, 2, 3]).unsqueeze(0)\n",
    "    result_with_match = find_candidate_pred_tokens(with_match, max_ngram_size=3, num_pred_tokens=3)\n",
    "    assert torch.equal(result_with_match, torch.tensor([1, 2, 3]))\n",
    "\n",
    "    # Test Case 2: Matching ngram is not found\n",
    "    without_match = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).unsqueeze(0)\n",
    "    result_without_match = find_candidate_pred_tokens(without_match, max_ngram_size=3, num_pred_tokens=3)\n",
    "    # For an empty result, ensure the result is an empty tensor of the expected shape or type\n",
    "    assert torch.equal(result_without_match, torch.tensor([]))\n",
    "\n",
    "test_find_candidate_pred_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
