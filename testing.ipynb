{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPT, GPTConfig\n",
    "import torch\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(model_name)\n",
    "\n",
    "def encode(string):\n",
    "    return tokenizer.encode(string, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "def decode(string):\n",
    "    return tokenizer.decode(string)\n",
    "\n",
    "def encode_tensor(string, device):\n",
    "    return torch.tensor(encode(string), dtype=torch.long, device=device)\n",
    "\n",
    "prompt = \"Hello, my name is Martin\"\n",
    "prompt_tensor = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nanogpt model (master branch)\n",
    "#model = GPT.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.65M\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# baseline model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT(GPTConfig(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50257\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/gpt2.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# baseline model\n",
    "model = GPT(GPTConfig(vocab_size=50257))\n",
    "model.load_state_dict(torch.load('models/gpt2.pth'))\n",
    "model.to(device, dtype=torch.bfloat16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Martin. I am a member of the United States Army\n",
      "Number of tokens: 16\n",
      "Time taken: 0.06 seconds\n",
      "Tokens per second: 178\n",
      "tensor([[15496,    11,   616,  1438,   318,  5780,    13,   314,   716,   257,\n",
      "          2888,   286,   262,  1578,  1829,  5407]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#run generation\n",
    "\n",
    "start_time = time.time()\n",
    "completion = model.generate(prompt_tensor, 10, greedy=True)\n",
    "end_time = time.time()\n",
    "\n",
    "completion_tokens = completion[0].cpu().tolist()\n",
    "num_tokens = len(completion_tokens)\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "tokens_per_second = (num_tokens-len(prompt_tensor.squeeze(0))) / time_taken\n",
    "\n",
    "print(decode(completion_tokens))\n",
    "print(f\"Number of tokens: {num_tokens}\")\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "print(f\"Tokens per second: {int(tokens_per_second)}\")\n",
    "print(completion)\n",
    "#float32\n",
    "#Hello, my name is Martin. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times.\n",
    "#bfloat16\n",
    "#Hello, my name is Martin. I am a member of the United States Army, and I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 50257])\n",
      "torch.Size([1, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[-56.7500]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([[13]], device='cuda:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline is 200 tokens per second (210 int8), 180 with sampling (160 int8)\n",
    "\n",
    "#bfloat16\n",
    "logits=model(prompt_tensor)[0]\n",
    "print(logits.shape)\n",
    "print(logits[:, -1, :].shape)\n",
    "torch.max(logits[:, -1, :], dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "from generate import load_model\n",
    "\n",
    "prompt_tensor = encode_tensor(prompt, device)\n",
    "\n",
    "precision = torch.bfloat16\n",
    "device=\"cuda\"\n",
    "model_path = \"models/gpt2.pth\"\n",
    "model = load_model(model_path, device, precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([15496,    11,   616,  1438,   318,  5780,    13,   314,   716,   257,\n",
       "          2888,   286,   262,  1578,  1829,  5407,    11], device='cuda:0'),\n",
       " {'accept_counts': [0, 0, 0, 0, 0, 0, 0, 0, 0]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate import generate\n",
    "generate(model, torch.cat([prompt_tensor, torch.tensor([13], device=device)]), 10, draft_model=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenneth/anaconda3/envs/pytorch310/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:124: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 100\n",
      "Time taken: 14.33 seconds\n",
      "Tokens per second: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([15496,    11,   616,  1438,   318,  5780,    13,   314,  1101,   716,\n",
       "          198,     9,     9,     9,    10,    59,    11,    14,    11,    11,\n",
       "           11,    11,    11,    11,    11,    11,   357,    25,    13,    11,\n",
       "           26,    27,    27,    28,    28,    28,    13,    13,    21,    25,\n",
       "           25,    28,    13,    41,    41,    13,    48,    41,    48,    41,\n",
       "           41,    48,    41,    41,    48,    41,    41,    41,    41,    52,\n",
       "           31,    31,    40,    55,    62,    62,    62,    28,    36,    28,\n",
       "           38,    39,    25,    13,    74,    62,    44,    62,    46,    62,\n",
       "           80,    81,    85,    62,    52,    86,    86,    55,    86,    89,\n",
       "           58,    55,    62,    62,    53,    53,    58,    58,    30,    30,\n",
       "           28,    28,    25,    25,    25,    13,    27], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate import run_generation\n",
    "#torch._dynamo.config.guard_nn_modules=True\n",
    "#https://github.com/huggingface/transformers/issues/25420\n",
    "run_generation(torch.cat([prompt_tensor, torch.tensor([13], device=device)]), \n",
    "               model, \n",
    "               100, \n",
    "               compile=True, \n",
    "               compile_prefill=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 100\n",
      "Time taken: 0.07 seconds\n",
      "Tokens per second: 1338\n",
      "Wikis are enabled by wiki software, otherwise known as wiki engines. A wiki engine, being a form of a content management system, differs from other web-based systems such as blog software or static site generators, in that the content is created without any defined owner or leader, and wikis have little inherent structure, allowing structure to emerge according to the needs of the users. This is evidenced by the numerous examples of websites that use Wikipedia as their website, and many other examples of articles, articles, or videos that are written by people who do not write for Wikipedia.\n",
      "\n",
      "The importance of a wiki in creating a viable community\n",
      "\n",
      "Wikipedia's existence is very much a part of what makes it a real asset to a community. Because of this, the Wikimedia Foundation has long been known for its commitment to building the best website for the history of the web, with a\n"
     ]
    }
   ],
   "source": [
    "from generate import run_generation\n",
    "\n",
    "txt=\"Wikis are enabled by wiki software, otherwise known as wiki engines. A wiki engine, being a form of a content management system, differs from other web-based systems such as blog software or static site generators, in that the content is created without any defined owner or leader, and wikis have little inherent structure, allowing structure to emerge according to the needs of the users.\"\n",
    "prompt_tensor = encode_tensor(txt, device)\n",
    "\n",
    "x=run_generation(prompt_tensor, \n",
    "               model, \n",
    "               100, \n",
    "               compile=True, \n",
    "               compile_prefill=True)\n",
    "print(decode(x.cpu().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenneth/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/kenneth/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/home/kenneth/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 256\n",
      "Time taken: 14.74 seconds\n",
      "Tokens per second: 17\n",
      "Hello, my name is Martin.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "x = run_generation(prompt_tensor, model, compile=True)\n",
    "print(decode(x.cpu().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPT\n",
    "import tiktoken\n",
    "import torch\n",
    "import time\n",
    "\n",
    "device = \"mps\"\n",
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "number of parameters: 123.65M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenneththoromartinsen/.pyenv/versions/3.10.14/envs/nanogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = GPT.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(model_name)\n",
    "\n",
    "def encode(string):\n",
    "    return tokenizer.encode(string, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "def decode(string):\n",
    "    return tokenizer.decode(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time (+/- SD): 1.51 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "prompt = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus eget purus metus. Nullam efficitur, sem ac facilisis tempor, urna quam pharetra tellus, ut mollis tellus quam vitae libero. Sed faucibus posuere varius. Morbi sit amet congue ex. Fusce vitae tellus sem. Donec dignissim hendrerit laoreet. Vestibulum mattis fringilla bibendum. Aenean pharetra felis libero, ac hendrerit libero porttitor sed. Suspendisse fermentum, ante sit amet faucibus tincidunt, libero quam mollis ex, non accumsan massa eros ac metus. Praesent leo risus, finibus at pretium et, venenatis ac ex. Sed posuere quam vitae turpis volutpat, vel volutpat augue dignissim. Donec hendrerit pretium mattis. Morbi tempus tellus at dolor ornare lobortis. Aenean tempor ligula cursus magna molestie venenatis. Ut feugiat semper lorem sed fringilla. Etiam pharetra, nisl a pellentesque iaculis, felis mi ultricies mi, quis feugiat elit erat at magna.\n",
    "\n",
    "Summary:\n",
    "'''\n",
    "\n",
    "prompt_tensor = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "#repeat sampling 6 times and save timings for 5 runs (in case it compiles on the first run)\n",
    "times = []\n",
    "texts = []\n",
    "for _ in range(6):\n",
    "    start = time.time()\n",
    "    completion = model.generate_greedy(prompt_tensor, max_new_tokens=50)\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "    texts.append(decode(completion[0].cpu().tolist()))\n",
    "\n",
    "assert all(text == texts[0] for text in texts[1:])\n",
    "\n",
    "time_mean = torch.tensor(times[1:]).mean()\n",
    "time_sd = torch.tensor(times[1:]).std()\n",
    "\n",
    "print(f\"Mean time (+/- SD): {time_mean:.2f} (+/- {time_sd:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_candidate_pred_tokens(input_ids: torch.Tensor, max_ngram_size: int = 3, num_pred_tokens: int = 10) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Finds candidate prediction tokens based on the input_ids.\n",
    "\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): The input tensor of shape (batch_size, seq_len) containing token IDs.\n",
    "        max_ngram_size (int, optional): The maximum size of the n-gram to search for. Defaults to 3.\n",
    "        num_pred_tokens (int, optional): The number of prediction tokens to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The tensor containing the candidate prediction tokens.\n",
    "    \"\"\"\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:end_idx]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='mps:0', dtype=torch.int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_candidate_pred_tokens(prompt_tensor, max_ngram_size=3, num_pred_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_find_candidate_pred_tokens():\n",
    "\n",
    "    # Test Case 1: Matching ngram is found\n",
    "    with_match = torch.tensor([1, 2, 3, 1, 2, 3, 1, 2, 3]).unsqueeze(0)\n",
    "    result_with_match = find_candidate_pred_tokens(with_match, max_ngram_size=3, num_pred_tokens=3)\n",
    "    assert torch.equal(result_with_match, torch.tensor([1, 2, 3]))\n",
    "\n",
    "    # Test Case 2: Matching ngram is not found\n",
    "    without_match = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).unsqueeze(0)\n",
    "    result_without_match = find_candidate_pred_tokens(without_match, max_ngram_size=3, num_pred_tokens=3)\n",
    "    # For an empty result, ensure the result is an empty tensor of the expected shape or type\n",
    "    assert torch.equal(result_without_match, torch.tensor([]))\n",
    "\n",
    "test_find_candidate_pred_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
