{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPT, GPTConfig\n",
    "import torch\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(model_name)\n",
    "\n",
    "def encode(string):\n",
    "    return tokenizer.encode(string, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "def decode(string):\n",
    "    return tokenizer.decode(string)\n",
    "\n",
    "def encode_tensor(string, device):\n",
    "    return torch.tensor(encode(string), dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nanogpt model (master branch)\n",
    "#model = GPT.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#run generation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(prompt_tensor, \u001b[38;5;241m10\u001b[39m, greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m completion_tokens \u001b[38;5;241m=\u001b[39m completion[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch310/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "# baseline model (master branch)\n",
    "model = GPT(GPTConfig(vocab_size=50257))\n",
    "model.load_state_dict(torch.load('models/gpt2.pth'))\n",
    "model.to(device, dtype=torch.bfloat16)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Hello, my name is Martin\"\n",
    "prompt_tensor = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "#run generation\n",
    "start_time = time.time()\n",
    "completion = model.generate(prompt_tensor, 10, greedy=True)\n",
    "end_time = time.time()\n",
    "\n",
    "completion_tokens = completion[0].cpu().tolist()\n",
    "num_tokens = len(completion_tokens)\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "tokens_per_second = (num_tokens-len(prompt_tensor.squeeze(0))) / time_taken\n",
    "\n",
    "print(decode(completion_tokens))\n",
    "print(f\"Number of tokens: {num_tokens}\")\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "print(f\"Tokens per second: {int(tokens_per_second)}\")\n",
    "print(completion)\n",
    "\n",
    "#float32\n",
    "#Hello, my name is Martin. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times. I'm a writer and a writer's assistant at the New York Times.\n",
    "#bfloat16\n",
    "#Hello, my name is Martin. I am a member of the United States Army, and I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am a member of the United States Army Reserve. I am\n",
    "\n",
    "#baseline is 200 tokens per second (210 int8), 180 with sampling (160 int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "from generate import load_model\n",
    "\n",
    "precision = torch.bfloat16\n",
    "device=\"cuda\"\n",
    "model_path = \"models/gpt2.pth\"\n",
    "model = load_model(model_path, device, precision, strict=True)\n",
    "\n",
    "#model_path_8int = \"models/gpt2-int8.pth\"\n",
    "#model = load_model(model_path_8int, device, precision, strict=False)\n",
    "\n",
    "prompt = \"Wikis are enabled by wiki software, otherwise known as wiki engines. A wiki engine, being a form of a content management system, differs from other web-based systems such as blog software or static site generators, in that the content is created without any defined owner or leader, and wikis have little inherent structure, allowing structure to emerge according to the needs of the users.\"\n",
    "prompt_tensor = encode_tensor(prompt, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 250\n",
      "Time taken: 12.07 seconds\n",
      "Tokens per second: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([33010,   271,   389,  9343,   416, 22719,  3788,    11,  4306,  1900,\n",
       "          355, 22719, 11874,    13,   317, 22719,  3113,    11,   852,   257,\n",
       "         1296,   286,   257,  2695,  4542,  1080,    11, 24242,   422,   584,\n",
       "         3992,    12,  3106,  3341,   884,   355,  4130,  3788,   393,  9037,\n",
       "         2524, 27298,    11,   287,   326,   262,  2695,   318,  2727,  1231,\n",
       "          597,  5447,  4870,   393,  3554,    11,   290, 47145,   271,   423,\n",
       "         1310, 11519,  4645,    11,  5086,  4645,   284, 14740,  1864,   284,\n",
       "          262,  2476,   286,   262,  2985,    13,   383, 22719,  3113,   318,\n",
       "         3170,  2402,   262,  3721,   286,   257, 22719,    11,   290,  3407,\n",
       "          257,  6831,   286,  3696,    11, 20150,   290,  6117,    13,   383,\n",
       "         6831,    11,   618,   973,   329,  2695,  4542,    11,  3407,  1366,\n",
       "          884,   355,  2836,  2106,    11,  2836, 16545,    11,  2836,  2989,\n",
       "         2106,    11,  2695,  9238,    11,   290,   257,  2792,   284,   584,\n",
       "         6685,   319,   262, 22719,    13, 50256,    40,   423,  4271,   531,\n",
       "          326,   262,  1266,   835,   284,  1805,  3511,   318,   284,   423,\n",
       "          257,  1598,  5761,   287,   534,  2000,    11,   475,   314,   892,\n",
       "          612,   318,   257,  1256,   286, 10802,   290, 32805,   287,   262,\n",
       "         1294,   546,   644,   366, 20063,  5761,     1,  1724,    13,   554,\n",
       "         3950,   340,   318,   257,  6087,   286,   734,  1243,    25,   198,\n",
       "          198,    17,     8,  1081,  5081,  2029,    11,   262,  6770,   286,\n",
       "         1598,  5761,   318,  1912,  2402, 19088,  1998,    13,   770,  6770,\n",
       "          318,   407,  1912,   319,  1997,   356,   466,   393,   910,    13,\n",
       "         4900,   326,  3729,  5238,   588, 10802,   284,   502,    11,   340,\n",
       "          318,  1682,   845,  4465,   329,   661,   508,   389,  2045,   329,\n",
       "          257,  2176,  6770,   286, 11459, 19009,   284,   466,   257,  1310,\n",
       "         2267,   319,   644,   340,  1724,    13,   198,   198,  5962,   286,\n",
       "          477,    11,   644,   318,   366, 20063,  5761, 13984,   198,   198,\n",
       "            1, 19856,  5761,     1,   318,   281, 25510,  3381,   326, 38932,\n",
       "          262,  1708,    25,   198,   198,    16,     8, 14173, 11365,    11,\n",
       "          198,   198,    17,     8, 14173, 11365,   284,  1657,   357,  2971,\n",
       "          852,  1498,   284, 28302,   832,  4168,   828,   198,   198,    18,\n",
       "            8, 14173, 11365,   284, 31094,  7032,   357, 31067,   414,   852,\n",
       "         1498,   284,  3802,   262, 46005,   828], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate import run_generation\n",
    "#https://github.com/huggingface/transformers/issues/25420\n",
    "\n",
    "run_generation(prompt_tensor,\n",
    "               model, \n",
    "               250, \n",
    "               compile=True, \n",
    "               compile_prefill=True,\n",
    "               temperature=0.7,\n",
    "               top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 250\n",
      "Time taken: 0.28 seconds\n",
      "Tokens per second: 904\n",
      "\n",
      "\n",
      "\n",
      "The wikis developed by wikis rely on various mechanisms, to define, and to organize wikis, the properties of wikis, and to set the rules for wikis. A wikis is the collective work of many wikis.\n",
      "\n",
      "\n",
      "The wikis are organized in a way that allows different groups to work together, without the need to create a separate wiki.\n",
      "\n",
      "The wiki is organized into a hierarchy of members, each with their own set of attributes, and members are allowed to edit, publish, and edit wikis. The wiki is organized into its own wiki, which is a \"link\" to a wiki's own content.\n",
      "\n",
      "\n",
      "A wiki's wiki is a set of documents and records relating to the wiki, which may contain information about the wiki as well as articles on the wiki. The wiki does not have any \"self-contained\" content.\n",
      "\n",
      "\n",
      "The wikis are organized as a set of rules or guidelines which are maintained by the wikis, and enforced by them. The wikis are organized as a set of guidelines or guidelines that are maintained by the wikis, and enforced by them. The wiki, like all hierarchical systems, has no rules and should not be used as a guideline\n"
     ]
    }
   ],
   "source": [
    "output = run_generation(prompt_tensor, \n",
    "                        model, \n",
    "                        250, \n",
    "                        compile=True, \n",
    "                        compile_prefill=True,\n",
    "                        temperature=0.7,\n",
    "                        top_k=50)\n",
    "\n",
    "prompt_len = len(prompt_tensor)\n",
    "\n",
    "print(decode(output.cpu().tolist()[prompt_len:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_candidate_pred_tokens(input_ids: torch.Tensor, max_ngram_size: int = 3, num_pred_tokens: int = 10) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Finds candidate prediction tokens based on the input_ids.\n",
    "\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): The input tensor of shape (batch_size, seq_len) containing token IDs.\n",
    "        max_ngram_size (int, optional): The maximum size of the n-gram to search for. Defaults to 3.\n",
    "        num_pred_tokens (int, optional): The number of prediction tokens to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The tensor containing the candidate prediction tokens.\n",
    "    \"\"\"\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:end_idx]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='mps:0', dtype=torch.int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_candidate_pred_tokens(prompt_tensor, max_ngram_size=3, num_pred_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_find_candidate_pred_tokens():\n",
    "\n",
    "    # Test Case 1: Matching ngram is found\n",
    "    with_match = torch.tensor([1, 2, 3, 1, 2, 3, 1, 2, 3]).unsqueeze(0)\n",
    "    result_with_match = find_candidate_pred_tokens(with_match, max_ngram_size=3, num_pred_tokens=3)\n",
    "    assert torch.equal(result_with_match, torch.tensor([1, 2, 3]))\n",
    "\n",
    "    # Test Case 2: Matching ngram is not found\n",
    "    without_match = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).unsqueeze(0)\n",
    "    result_without_match = find_candidate_pred_tokens(without_match, max_ngram_size=3, num_pred_tokens=3)\n",
    "    # For an empty result, ensure the result is an empty tensor of the expected shape or type\n",
    "    assert torch.equal(result_without_match, torch.tensor([]))\n",
    "\n",
    "test_find_candidate_pred_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
